{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NICE_filled.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:03.465010Z",
          "start_time": "2019-10-28T11:04:02.932529Z"
        },
        "colab_type": "code",
        "id": "VHA-vuH9Ox80",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import distributions\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "id": "kh447DNxs1W5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "8eE_AFMWs1W7",
        "colab_type": "text"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:03.474775Z",
          "start_time": "2019-10-28T11:04:03.467368Z"
        },
        "colab_type": "code",
        "id": "qAf9BJpoOx83",
        "colab": {}
      },
      "source": [
        "def plot_imgs(samples, title=None):\n",
        "    fig, ax = plt.subplots(2, 5, figsize=(10,4))\n",
        "    fig.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
        "    plt.rcParams.update({'font.size': 20})\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(5):\n",
        "            ax[i, j].imshow(samples[i*5 + j], cmap='gray')\n",
        "            ax[i, j].axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:03.637436Z",
          "start_time": "2019-10-28T11:04:03.623017Z"
        },
        "colab_type": "code",
        "id": "afHxhu33Ox85",
        "colab": {}
      },
      "source": [
        "def load_data(data_dir, dataset):\n",
        "    if dataset == 'MNIST':\n",
        "        if not os.path.exists(data_dir):\n",
        "            os.makedirs(data_dir)\n",
        "        points = torchvision.datasets.MNIST(root=data_dir, train=True,\n",
        "                                               transform=transforms.ToTensor(),download=True)\n",
        "        \n",
        "    elif dataset == 'MOONS':\n",
        "        moon1 = [(0.5 + r*np.cos(t), r*np.sin(t)) \n",
        "                 for t in np.arange(0, np.pi, 0.01) for r in np.arange(0.9, 1.1, 0.01)]\n",
        "        moon2 = [(-0.5 + r*np.cos(t), r*np.sin(t)) \n",
        "                 for t in np.arange(np.pi, 2*np.pi, 0.01) for r in np.arange(0.9, 1.1, 0.01)]\n",
        "        points = moon1 + moon2\n",
        "        points = torch.tensor(points)\n",
        "        \n",
        "    elif dataset == 'MLINPL':\n",
        "        img = cv2.imread(data_dir + r'MLinPL.png',0) / 255\n",
        "        n, m = img.shape\n",
        "        n, m = int(0.2*n), int(0.2*m)\n",
        "        img = cv2.resize(img, (m,n))\n",
        "\n",
        "        points = []\n",
        "        for i in range(n):\n",
        "            for j in range(m):\n",
        "                if img[i,j] == 0:\n",
        "                    points.append((j,-i))\n",
        "\n",
        "        points = np.array(points)\n",
        "        points = (points - np.mean(points, axis=0, keepdims=True)) / np.std(points, axis=0, keepdims=True)\n",
        "        points = torch.from_numpy(points)\n",
        "        \n",
        "    return points"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "RwF7z4Nis1W_",
        "colab_type": "text"
      },
      "source": [
        "# Model implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "tgtJNzuds1XA",
        "colab_type": "text"
      },
      "source": [
        "Implement Coupling layer of the NICE model here. You will need to fill the body of 3 functions: \n",
        "* `forward` - forward pass of the NICE\n",
        "$$\\begin{cases}\n",
        "y_1 =& x_1\\\\ \n",
        "y_2 =& x_2 + m(x_1)\n",
        "\\end{cases}$$\n",
        "* `inverse` - inversion of the forward pass\n",
        "$$\\begin{cases}\n",
        "x_1 =& y_1\\\\ \n",
        "x_2 =& y_2 - m(x_1)\n",
        "\\end{cases}$$\n",
        "* `get_mask` - swap of the processed part of latent code\n",
        "$$\\begin{cases}\n",
        "y_{I_1} =& y_{I_2}\\\\ \n",
        "y_{I_2} =& y_{I_1}\n",
        "\\end{cases}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:04.146951Z",
          "start_time": "2019-10-28T11:04:04.131525Z"
        },
        "colab_type": "code",
        "id": "nNDRmb_-Ox87",
        "colab": {}
      },
      "source": [
        "class Coupling_layer(nn.Module):        \n",
        "    def __init__(self, device, input_dim, data_dim, n_layers, mask_type, hidden_dim=1024):\n",
        "        super(Coupling_layer, self).__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        self.mask = self.get_mask(data_dim, mask_type)\n",
        "        \n",
        "        net = [nn.Linear(input_dim, hidden_dim), nn.ReLU()]\n",
        "        for i in range(n_layers-2):\n",
        "            net.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            net.append(nn.ReLU())\n",
        "        net.append(nn.Linear(hidden_dim, input_dim))\n",
        "        self.net = nn.Sequential(*net)\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        z = x.view(x.shape[0], -1)\n",
        "        h1, h2 = z * self.mask, z * (1 - self.mask)\n",
        "        \n",
        "        #h1 = h1\n",
        "        h2 = h2 + self.net(h1) * (1 - self.mask)\n",
        "        \n",
        "        z = h1 + h2\n",
        "        \n",
        "        return z.view(x.shape)\n",
        "    \n",
        "    \n",
        "    def inverse(self, z):\n",
        "        x = z.view(z.shape[0], -1)\n",
        "        h1, h2 = x * self.mask, x * (1 - self.mask)\n",
        "        \n",
        "        #h1 = h1\n",
        "        h2 = h2 - self.net(h1) * (1 - self.mask)\n",
        "        \n",
        "        x = h1 + h2 \n",
        "        \n",
        "        return x.view(z.shape)\n",
        "    \n",
        "    \n",
        "    def get_mask(self, data_dim, mask_type: int):\n",
        "        # Return binary (0 and 1) mask with 1's in first half for mask_type=0, and in the second half for mask_type=1.\n",
        "        \n",
        "        self.mask = torch.zeros(data_dim)\n",
        "        \n",
        "        if mask_type == 0:\n",
        "            self.mask[:,:data_dim[1]//2] = 1\n",
        "            \n",
        "        elif mask_type == 1:\n",
        "            self.mask[:,data_dim[1]//2:] = 1\n",
        "            \n",
        "        return self.mask.view(1,-1).to(self.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "jJpxtYQXs1XC",
        "colab_type": "text"
      },
      "source": [
        "Implement scaling layer here. You will need a body for 2 functions:\n",
        "* `forward` - forward pass of the layer. Return scaled output and logarithm of the determinant.\n",
        "$$ \\text{out} = (x * \\exp(\\log s), \\log \\det s)$$\n",
        "If we keep logarithms of the scales $\\log s$ then: \n",
        "$$\\log \\det s = \\sum_{i=0}^{n} s_i,$$\n",
        "where $n$ is the legnth of the diagonal.\n",
        "* `inverse` - inversion of the scaling layer\n",
        "$$\\text{out} = z * \\exp(-\\log s)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:04.753996Z",
          "start_time": "2019-10-28T11:04:04.747356Z"
        },
        "colab_type": "code",
        "id": "5PdqwWqtOx89",
        "colab": {}
      },
      "source": [
        "class Scaling_layer(nn.Module):        \n",
        "    def __init__(self, data_dim):\n",
        "        super(Scaling_layer, self).__init__()\n",
        "        \n",
        "        self.log_scale = nn.Parameter(torch.FloatTensor(data_dim).fill_(0.0))\n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        scale_logdet = torch.sum(self.log_scale)\n",
        "        return torch.exp(self.log_scale) * x, scale_logdet\n",
        "    \n",
        "    \n",
        "    def inverse(self, z):\n",
        "        return torch.exp(-self.log_scale) * z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "Z37AsqgKs1XF",
        "colab_type": "text"
      },
      "source": [
        "Here, you will need to implement two functions, responsible for calculating flow and inverse flow:\n",
        "* `flow` - compositionn of coupling layers on an input $x$ - $f_K(f_{K-1}(\\ldots f_1(x)))$ and scaling layer at the end. Returns $z$ and $\\log \\det s$\n",
        "* `inv_flow` - compositionn of inversion and scaling layer and inversion of coupling layers on an encoding $z$. Returns $x$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:05.273681Z",
          "start_time": "2019-10-28T11:04:05.258085Z"
        },
        "colab_type": "code",
        "id": "0eqNhPywOx8_",
        "colab": {}
      },
      "source": [
        "class NICE():\n",
        "    def __init__(self, input_dim, data_dim, n_layers, n_couplings, device):\n",
        "        self.coupling_layers = []\n",
        "        for i in range(n_couplings):\n",
        "            if i%2 == 0:\n",
        "                self.coupling_layers.append(Coupling_layer(device, input_dim, data_dim, n_layers, 0).to(device))\n",
        "            else:\n",
        "                self.coupling_layers.append(Coupling_layer(device, input_dim, data_dim, n_layers, 1).to(device))\n",
        "        self.scaling_layer = Scaling_layer(data_dim).to(device)\n",
        "    \n",
        "    \n",
        "    def flow(self, x):\n",
        "        for layer in self.coupling_layers:\n",
        "            x = layer(x)\n",
        "        z, scale_logdet = self.scaling_layer(x)\n",
        "        \n",
        "        return z, scale_logdet\n",
        "    \n",
        "    \n",
        "    def inv_flow(self, z):\n",
        "        x = self.scaling_layer.inverse(z)\n",
        "        for layer in self.coupling_layers[::-1]:\n",
        "            x = layer.inverse(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    \n",
        "    def init_weights(self):\n",
        "        for layer in self.coupling_layers:\n",
        "            layer.apply(self.init_weights_helper)\n",
        "    \n",
        "    \n",
        "    def init_weights_helper(self, Layer):\n",
        "        name = Layer.__class__.__name__\n",
        "        if name == 'Linear':\n",
        "            torch.nn.init.normal_(Layer.weight, mean=0, std=0.02)\n",
        "            if Layer.bias is not None:\n",
        "                torch.nn.init.constant_(Layer.bias, 0)\n",
        "    \n",
        "    \n",
        "    def get_parameters(self):\n",
        "        parameters = []\n",
        "        for layer in self.coupling_layers:\n",
        "            parameters += list(layer.parameters())\n",
        "        parameters += list(self.scaling_layer.parameters())\n",
        "        \n",
        "        return parameters\n",
        "    \n",
        "    \n",
        "    def train_model(self, if_train=True):\n",
        "        if if_train:\n",
        "            for layer in self.coupling_layers:\n",
        "                layer.train()\n",
        "            self.scaling_layer.train()\n",
        "        else:\n",
        "            for layer in self.coupling_layers:\n",
        "                layer.eval()\n",
        "            self.scaling_layer.eval()            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:05.981199Z",
          "start_time": "2019-10-28T11:04:05.976692Z"
        },
        "colab_type": "code",
        "id": "Xy3J2RfvOx9B",
        "colab": {}
      },
      "source": [
        "def loss_fun(z, prior_z, logdet):\n",
        "    z = z.view(z.shape[0], -1)\n",
        "    ll_z = prior_z.log_prob(z.cpu()).to(device) + logdet\n",
        "    return -torch.mean(ll_z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "ovHsPfQgs1XJ",
        "colab_type": "text"
      },
      "source": [
        "# Experiments\n",
        "\n",
        "Here we perform experiments on 3 datasets: MNIST, Moons and MLinPL logo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-21T15:03:55.024866Z",
          "start_time": "2019-10-21T14:53:51.839Z"
        },
        "colab_type": "text",
        "id": "PY_m2G3TOx9D"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "t_fuidLts1XK",
        "colab_type": "text"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:08.317349Z",
          "start_time": "2019-10-28T11:04:06.771429Z"
        },
        "colab_type": "code",
        "id": "NIC-9RdLOx9E",
        "colab": {}
      },
      "source": [
        "mnist = load_data(r'datasets/', 'MNIST')\n",
        "\n",
        "mnist.data = (mnist.data.float() / 255. - 0.1307) / 0.3081\n",
        "data = mnist.data\n",
        "targets = mnist.targets\n",
        "data = data[[idx for idx in range(len(targets)) if targets[idx] in [0,1,2,3,4]]]\n",
        "targets = targets[[idx for idx in range(len(targets)) if targets[idx] in [0,1,2,3,4]]]\n",
        "\n",
        "dataloader = DataLoader(data, batch_size=128, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "NNMsIECjs1XN",
        "colab_type": "text"
      },
      "source": [
        "Model setup. Here we setup the prior distribution as a Multivariate Isometric Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:04:11.151085Z",
          "start_time": "2019-10-28T11:04:08.319903Z"
        },
        "colab_type": "code",
        "id": "AsWJ-pboOx9G",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "n_epochs = 1000\n",
        "l_rate = 1e-3\n",
        "n_layers = 6 # in each coupling layer\n",
        "n_couplings = 6\n",
        "\n",
        "data_dim = data.shape[1:]\n",
        "input_dim = torch.prod(torch.tensor(data.shape[1:])).item()\n",
        "\n",
        "prior_z = distributions.MultivariateNormal(torch.zeros(input_dim), torch.eye(input_dim))\n",
        "\n",
        "nice = NICE(input_dim, data_dim, n_layers, n_couplings, device)\n",
        "nice.init_weights()\n",
        "\n",
        "optimizer = torch.optim.Adam(nice.get_parameters(), lr=l_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "rxv8xMLCs1XP",
        "colab_type": "text"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T11:54:16.580571Z",
          "start_time": "2019-10-28T11:04:14.601900Z"
        },
        "colab_type": "code",
        "id": "ixWEca52Ox9H",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "nice.train_model()\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    loss_acc = 0\n",
        "    for j, x in enumerate(dataloader):\n",
        "        x = (x.float() + torch.rand(x.shape)).to(device) / 255\n",
        "        z, logdet = nice.flow(x)\n",
        "        loss = loss_fun(z, prior_z, logdet)\n",
        "        loss_acc += loss.item()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    scheduler.step()\n",
        "    \n",
        "    if i%5 == 0:\n",
        "        print('Epoch: {}/{} Loss: {:.4f}'.format(i+1, n_epochs, loss_acc / (j+1)))\n",
        "        with torch.no_grad():\n",
        "            samples = torch.randn(10,28,28).to(device)\n",
        "            samples = nice.inv_flow(samples)\n",
        "            plot_imgs(samples.cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "9KFCI-5Bs1XS",
        "colab_type": "text"
      },
      "source": [
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T12:53:40.413820Z",
          "start_time": "2019-10-28T12:53:40.023042Z"
        },
        "colab_type": "code",
        "id": "K0miz7zkOx9O",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "nice.train_model(False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    samples = torch.randn(10,28,28).to(device)\n",
        "    samples = nice.inv_flow(samples)\n",
        "    plot_imgs(samples.cpu().numpy(), None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "n_prP08POx9Q"
      },
      "source": [
        "## Moons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "e1Qjc7e4s1XU",
        "colab_type": "text"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T12:53:48.355683Z",
          "start_time": "2019-10-28T12:53:48.108374Z"
        },
        "colab_type": "code",
        "id": "EJRzWDuqOx9R",
        "colab": {}
      },
      "source": [
        "moons = load_data(r'datasets/', 'MOONS')\n",
        "\n",
        "plt.scatter(moons[:,0], moons[:,1], c='b')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "_yf6uORRs1XX",
        "colab_type": "text"
      },
      "source": [
        "Model setup. Here we setup the prior distribution as a Multivariate Isometric Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T12:53:48.967735Z",
          "start_time": "2019-10-28T12:53:48.740472Z"
        },
        "colab_type": "code",
        "id": "vWbstanOOx9T",
        "jupyter": {
          "source_hidden": true
        },
        "colab": {}
      },
      "source": [
        "n_epochs = 10000\n",
        "l_rate = 1e-3\n",
        "n_layers = 6 # in each coupling layer\n",
        "n_couplings = 6\n",
        "\n",
        "data_dim = (1,2)\n",
        "input_dim = 2\n",
        "\n",
        "prior_z = distributions.MultivariateNormal(torch.zeros(input_dim), torch.eye(input_dim))\n",
        "\n",
        "nice2D = NICE(input_dim, data_dim, n_layers, n_couplings, device)\n",
        "nice2D.init_weights()\n",
        "\n",
        "optimizer = torch.optim.Adam(nice2D.get_parameters(), lr=l_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "_LPMKE4hs1XZ",
        "colab_type": "text"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T13:24:32.970942Z",
          "start_time": "2019-10-28T13:24:32.900631Z"
        },
        "colab_type": "code",
        "id": "TfV5j09YOx9V",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "nice2D.train_model()\n",
        "moons = moons.float().to(device)\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    loss_acc = 0\n",
        "    x = moons + 1e-2 * torch.rand(moons.shape).to(device)\n",
        "    \n",
        "    z, logdet = nice2D.flow(x)\n",
        "    loss = loss_fun(z, prior_z, logdet)\n",
        "    loss_acc += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    \n",
        "    if i%50 == 0:\n",
        "        print('Epoch: {}/{} Loss: {:.4f}'.format(i+1, n_epochs, loss.item()))\n",
        "        with torch.no_grad():\n",
        "            samples = torch.randn(1000,2).to(device)\n",
        "            samples = nice2D.inv_flow(samples).view(-1,2)\n",
        "            plt.scatter(samples[:,0].cpu().numpy(), samples[:,1].cpu().numpy(), c='black')\n",
        "            plt.axis('off')\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "lrKXvFSfs1Xb",
        "colab_type": "text"
      },
      "source": [
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T13:22:53.298150Z",
          "start_time": "2019-10-28T13:22:53.007411Z"
        },
        "colab_type": "code",
        "id": "SfBCyhF9Ox9X",
        "colab": {}
      },
      "source": [
        "nice2D.train_model(False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    samples = torch.randn(1000,2).to(device)\n",
        "    samples = nice2D.inv_flow(samples).view(-1,2)\n",
        "    plt.scatter(samples[:,0].cpu().numpy(), samples[:,1].cpu().numpy(), c='black', alpha=0.1)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "colab_type": "text",
        "id": "D08vO-S6Ox9Z"
      },
      "source": [
        "## Sign"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "YyrcxHjts1Xg",
        "colab_type": "text"
      },
      "source": [
        "Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T13:23:49.007003Z",
          "start_time": "2019-10-28T13:23:48.899960Z"
        },
        "colab_type": "code",
        "id": "J-7AJf-0Ox9Z",
        "colab": {}
      },
      "source": [
        "points = load_data(r'images/', 'MLINPL')\n",
        "\n",
        "plt.scatter(points[:,0], points[:,1], c='b')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "-y_oVNYks1Xj",
        "colab_type": "text"
      },
      "source": [
        "Model setup. Here we setup the prior distribution as a Multivariate Isometric Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T13:23:50.882919Z",
          "start_time": "2019-10-28T13:23:50.503951Z"
        },
        "colab_type": "code",
        "id": "qEh9Gov8Ox9b",
        "colab": {}
      },
      "source": [
        "n_epochs = 10000\n",
        "l_rate = 1e-3\n",
        "n_layers = 6 # in each coupling layer\n",
        "n_couplings = 6\n",
        "\n",
        "data_dim = (1,2)\n",
        "input_dim = 2\n",
        "\n",
        "prior_z = distributions.MultivariateNormal(torch.zeros(input_dim), torch.eye(input_dim))\n",
        "\n",
        "nice_sign = NICE(input_dim, data_dim, n_layers, n_couplings, device)\n",
        "nice_sign.init_weights()\n",
        "\n",
        "optimizer = torch.optim.Adam(nice_sign.get_parameters(), lr=l_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=200, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "g2lLoFyjs1Xl",
        "colab_type": "text"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T13:24:12.466728Z",
          "start_time": "2019-10-28T13:23:51.839991Z"
        },
        "colab_type": "code",
        "id": "KoRrk8PpOx9d",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "nice_sign.train_model()\n",
        "points = points.float().to(device)\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    loss_acc = 0\n",
        "    x = points + 1e-4 * torch.rand(points.shape).to(device)\n",
        "    \n",
        "    z, logdet = nice_sign.flow(x)\n",
        "    loss = loss_fun(z, prior_z, logdet)\n",
        "    loss_acc += loss.item()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    \n",
        "    if i%50 == 0:\n",
        "        print('Epoch: {}/{} Loss: {:.4f}'.format(i+1, n_epochs, loss.item()))\n",
        "        with torch.no_grad():\n",
        "            samples = torch.randn(1000,2).to(device)\n",
        "            samples = nice_sign.inv_flow(samples).view(-1,2)\n",
        "            plt.scatter(samples[:,0].cpu().numpy(), samples[:,1].cpu().numpy(), c='black')\n",
        "            plt.axis('off')\n",
        "            plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "Collapsed": "false",
        "id": "TPWgu_0Gs1Xo",
        "colab_type": "text"
      },
      "source": [
        "Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "Collapsed": "false",
        "ExecuteTime": {
          "end_time": "2019-10-28T13:23:42.384543Z",
          "start_time": "2019-10-28T13:23:42.373617Z"
        },
        "colab_type": "code",
        "id": "5PBckVZLOx9f",
        "colab": {}
      },
      "source": [
        "nice_sign.train_model(False)\n",
        "\n",
        "with torch.no_grad():\n",
        "    samples = torch.randn(1000,2).to(device)\n",
        "    samples = nice_sign.inv_flow(samples).view(-1,2)\n",
        "    plt.scatter(samples[:,0].cpu().numpy(), samples[:,1].cpu().numpy(), c='black', alpha=0.1)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}